{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, GlobalAveragePooling1D, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAMES = 315\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Sign Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(parquet_path, feature_types = ['face','pose','hand']):\n",
    "    \n",
    "    data = pd.read_parquet(parquet_path)\n",
    "    USE_FACE = 'face' in feature_types \n",
    "    USE_POSE = 'pose' in feature_types\n",
    "    USE_HAND = 'hand' in feature_types\n",
    "\n",
    "    right_hand = data[data['type'].str.contains('right_hand')]\n",
    "    left_hand = data[data['type'].str.contains('left_hand')]\n",
    "    pose = data[data['type'].str.contains('pose')]\n",
    "    face = data[data['type'].str.contains('face')]\n",
    "\n",
    "    # Create lists where the element contain the x,y,z \n",
    "    # values for specific landmarks at each frame\n",
    "    right_hand_landmarks = []\n",
    "    left_hand_landmarks = []\n",
    "    pose_landmarks = []\n",
    "    face_landmarks = []\n",
    "    # merged contains all the landmarks that are requested \n",
    "    merged = []\n",
    "\n",
    "    # Get a list of all of the right hand landmarks by the landmark index\n",
    "    if USE_HAND:\n",
    "        for i in right_hand['landmark_index'].unique():\n",
    "            curr_right_hand_landmarks = pose[pose['landmark_index'] == i].copy()\n",
    "            curr_right_hand_landmarks.rename(\n",
    "                columns={\n",
    "                    \"x\": \"x_right_hand_\" + str(i),\n",
    "                    \"y\": \"y_right_hand_\" + str(i),\n",
    "                    \"z\": \"z_right_hand_\" + str(i),\n",
    "                },\n",
    "                inplace=True,\n",
    "            ) \n",
    "            curr_right_hand_landmarks.drop(\n",
    "                    [\"row_id\", \"type\", \"landmark_index\"], axis=1, inplace=True\n",
    "                )\n",
    "            curr_right_hand_landmarks.reset_index(drop=True, inplace=True)\n",
    "            curr_right_hand_landmarks.set_index(\"frame\", inplace=True)\n",
    "            right_hand_landmarks.append(curr_right_hand_landmarks)\n",
    "        # Get a list of all of the left hand landmarks by the landmark index\n",
    "        for i in left_hand['landmark_index'].unique():\n",
    "            curr_left_hand_landmarks = pose[pose['landmark_index'] == i].copy() \n",
    "            curr_left_hand_landmarks.rename(\n",
    "                columns={\n",
    "                    \"x\": \"x_left_hand_\" + str(i),\n",
    "                    \"y\": \"y_left_hand_\" + str(i),\n",
    "                    \"z\": \"z_left_hand_\" + str(i),\n",
    "                },\n",
    "                inplace=True,\n",
    "            ) \n",
    "            curr_left_hand_landmarks.drop(\n",
    "                    [\"row_id\", \"type\", \"landmark_index\"], axis=1, inplace=True\n",
    "                )\n",
    "            curr_left_hand_landmarks.reset_index(drop=True, inplace=True)\n",
    "            curr_left_hand_landmarks.set_index(\"frame\", inplace=True)\n",
    "            left_hand_landmarks.append(curr_left_hand_landmarks)\n",
    "            \n",
    "        merged_right_hand = pd.concat(right_hand_landmarks, axis = 1)\n",
    "        merged_left_hand = pd.concat(left_hand_landmarks, axis = 1)\n",
    "        \n",
    "        # Handle hand dominance\n",
    "        right_hand_nans = merged_right_hand.isna().sum().sum()\n",
    "        left_hand_nans = merged_left_hand.isna().sum().sum()\n",
    "\n",
    "        right_handed = left_hand_nans >= right_hand_nans\n",
    "        \n",
    "        if right_handed:\n",
    "            merged_right_hand.columns = merged_right_hand.columns.str.replace(\"_right\", \"\")\n",
    "            merged.append(merged_right_hand)\n",
    "        else: \n",
    "            center = 0.5\n",
    "            x_col_left = [col for col in merged_left_hand.columns if \"x\" in col]\n",
    "            for col in x_col_left:\n",
    "                merged_left_hand[col] = center - (merged_left_hand[col] - center)\n",
    "            merged_left_hand.columns = merged_left_hand.columns.str.replace(\"_left\", \"\")\n",
    "            merged.append(merged_left_hand)\n",
    "\n",
    "    # Get a list of all of the pose landmarks by the landmark index    \n",
    "    if USE_POSE:\n",
    "        for i in pose['landmark_index'].unique():\n",
    "            curr_pose_landmark = pose[pose['landmark_index'] == i].copy() \n",
    "            curr_pose_landmark.rename(\n",
    "                columns={\n",
    "                    \"x\": \"x_pose_\" + str(i),\n",
    "                    \"y\": \"y_pose_\" + str(i),\n",
    "                    \"z\": \"z_pose_\" + str(i),\n",
    "                },\n",
    "                inplace=True,\n",
    "            ) \n",
    "            curr_pose_landmark.drop(\n",
    "                    [\"row_id\", \"type\", \"landmark_index\"], axis=1, inplace=True\n",
    "                )\n",
    "            curr_pose_landmark.reset_index(drop=True, inplace=True)\n",
    "            curr_pose_landmark.set_index(\"frame\", inplace=True)\n",
    "            pose_landmarks.append(curr_pose_landmark)\n",
    "        merged_pose = pd.concat(pose_landmarks, axis = 1)\n",
    "        merged.append(merged_pose)\n",
    "        \n",
    "    # Get a list of all of the face landmarks by the landmark index\n",
    "    if USE_FACE:\n",
    "        for i in face['landmark_index'].unique():\n",
    "            curr_face_landmark = face[face['landmark_index'] == i].copy() \n",
    "            curr_face_landmark.rename(\n",
    "                columns={\n",
    "                    \"x\": \"x_face_\" + str(i),\n",
    "                    \"y\": \"y_face_\" + str(i),\n",
    "                    \"z\": \"z_face_\" + str(i),\n",
    "                },\n",
    "                inplace=True,\n",
    "            ) \n",
    "            curr_face_landmark.drop(\n",
    "                    [\"row_id\", \"type\", \"landmark_index\"], axis=1, inplace=True\n",
    "                )\n",
    "            curr_face_landmark.reset_index(drop=True, inplace=True)\n",
    "            curr_face_landmark.set_index(\"frame\", inplace=True)\n",
    "            face_landmarks.append(curr_face_landmark)\n",
    "        merged_face = pd.concat(face_landmarks, axis = 1)        \n",
    "        merged.append(merged_face)\n",
    "    \n",
    "    return pd.concat(merged, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the maximum number of frames in all samples for padding later\n",
    "def get_max_frames():\n",
    "    ROOT = 'data/filtered_data_5/'\n",
    "    file_paths = pd.read_csv('data/filtered_data_5/train.csv')['path']\n",
    "    frame_max = 0\n",
    "    for index, path in file_paths.items():\n",
    "        features = get_features(ROOT + path, ['pose'])\n",
    "        if len(features) > frame_max:\n",
    "            frame_max = len(features)\n",
    "    return frame_max\n",
    "# get_max_frames()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_map = {\n",
    "  \"55372\": 0, \"28656\": 1, \"53618\": 2,\n",
    "  \"62590\": 3, \"27610\": 4, \"37779\": 5,\n",
    "  \"4718\": 6,  \"25571\": 7, \"2044\": 8,\n",
    "  \"36257\": 9, \"29302\": 10, \"32319\": 11,\n",
    "  \"61333\": 12, \"16069\": 13, \"30680\": 14,\n",
    "  \"49445\": 15, \"18796\": 16, \"37055\": 17,\n",
    "  \"26734\": 18, \"22343\": 19, \"34503\": 20\n",
    "}\n",
    "\n",
    "sign_to_prediction_map = {\n",
    "    \"eye\": 0, \"gum\": 1, \"scissors\": 2, \"icecream\": 3, \"story\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_samples(original_df, desired_frames = MAX_FRAMES):\n",
    "    num_col = len(original_df.columns)\n",
    "    original_frames = len(original_df)\n",
    "    padding_before = (desired_frames - original_frames) // 2\n",
    "    padding_after = desired_frames - original_frames - padding_before\n",
    "    \n",
    "    if desired_frames <= original_frames:\n",
    "        start = original_frames / 2 - desired_frames / 2 \n",
    "        end = start + desired_frames\n",
    "        return original_df.iloc[start:end]\n",
    "    else: \n",
    "        pad_before_df = pd.DataFrame(\n",
    "            np.zeros((padding_before, num_col)), columns = original_df.columns\n",
    "        )\n",
    "        pad_after_df = pd.DataFrame(\n",
    "            np.zeros((padding_after, num_col)), columns = original_df.columns\n",
    "        )\n",
    "    return pd.concat([pad_before_df, original_df, pad_after_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_features():\n",
    "    POSE_FEATURE_ROOT = 'data/staged/pose_data/train_landmark_files'\n",
    "    RAW_ROOT = 'data/filtered_data_5/'\n",
    "    POSE_ROOT = 'data/staged/pose_data'\n",
    "    file_paths = pd.read_csv('data/filtered_data_5/train.csv')['path']\n",
    "    for _, path in file_paths.items():\n",
    "        parts = path.split('/')\n",
    "        participant = parts[1]\n",
    "        features = get_features(RAW_ROOT+path, ['pose'])\n",
    "        features = pad_samples(features)\n",
    "        full_path = os.path.join(POSE_ROOT, path)\n",
    "        if not os.path.exists(POSE_FEATURE_ROOT + '/' + participant):\n",
    "            os.makedirs(POSE_FEATURE_ROOT + '/' + participant)\n",
    "        features.to_parquet(full_path)\n",
    "\n",
    "def get_hand_features():\n",
    "    HAND_FEATURE_ROOT = 'data/staged/hand_data/train_landmark_files'\n",
    "    RAW_ROOT = 'data/filtered_data_5/'\n",
    "    HAND_ROOT = 'data/staged/hand_data'\n",
    "    file_paths = pd.read_csv('data/filtered_data_5/train.csv')['path']\n",
    "    for _, path in file_paths.items():\n",
    "        parts = path.split('/')\n",
    "        participant = parts[1]\n",
    "        features = get_features(RAW_ROOT+path, ['hand'])\n",
    "        features = pad_samples(features)\n",
    "        full_path = os.path.join(HAND_ROOT, path)\n",
    "        if not os.path.exists(HAND_FEATURE_ROOT + '/' + participant):\n",
    "            os.makedirs(HAND_FEATURE_ROOT + '/' + participant)\n",
    "        features.to_parquet(full_path)\n",
    "\n",
    "def get_hand_face_features():\n",
    "    HAND_FACE_FEATURE_ROOT = 'data/staged/hand_face_data/train_landmark_files'\n",
    "    RAW_ROOT = 'data/filtered_data_5/'\n",
    "    HAND_FACE_ROOT = 'data/staged/hand_face_data'\n",
    "    file_paths = pd.read_csv('data/filtered_data_5/train.csv')['path']\n",
    "    for _, path in file_paths.items():\n",
    "        parts = path.split('/')\n",
    "        participant = parts[1]\n",
    "        features = get_features(RAW_ROOT+path, ['hand','face'])\n",
    "        features = pad_samples(features)\n",
    "        full_path = os.path.join(HAND_FACE_ROOT, path)\n",
    "        if not os.path.exists(HAND_FACE_FEATURE_ROOT + '/' + participant):\n",
    "            os.makedirs(HAND_FACE_FEATURE_ROOT + '/' + participant)\n",
    "        features.to_parquet(full_path)\n",
    "        \n",
    "get_hand_face_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/55372/1304024428.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/55372/2261487237.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/55372/3544840292.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/55372/3566054186.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/55372/431106023.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>train_landmark_files/55372/1820892894.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>train_landmark_files/55372/3987453892.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>train_landmark_files/55372/2910954459.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>train_landmark_files/55372/4031658451.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>train_landmark_files/55372/2505475742.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path  class  train\n",
       "0     train_landmark_files/55372/1304024428.parquet      0      1\n",
       "1     train_landmark_files/55372/2261487237.parquet      0      1\n",
       "2     train_landmark_files/55372/3544840292.parquet      0      1\n",
       "3     train_landmark_files/55372/3566054186.parquet      0      1\n",
       "4      train_landmark_files/55372/431106023.parquet      0      1\n",
       "...                                             ...    ...    ...\n",
       "1045  train_landmark_files/55372/1820892894.parquet      4      1\n",
       "1046  train_landmark_files/55372/3987453892.parquet      4      1\n",
       "1047  train_landmark_files/55372/2910954459.parquet      4      1\n",
       "1048  train_landmark_files/55372/4031658451.parquet      4      1\n",
       "1049  train_landmark_files/55372/2505475742.parquet      4      1\n",
       "\n",
       "[1050 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_test_train():\n",
    "    files = pd.read_csv('data/filtered_data_5/train.csv')\n",
    "    def get_sign_class(sign):\n",
    "        return sign_to_prediction_map.get(sign, -1)\n",
    "    files['class'] = files['sign'].apply(get_sign_class)\n",
    "    \n",
    "    random_split = (list(range(21)))\n",
    "    random.shuffle(random_split)\n",
    "    test = random_split[:4]\n",
    "    train = random_split[4:]\n",
    "    \n",
    "    def train(participant_id):\n",
    "        if participant_map.get(str(participant_id), -1) in test:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1   \n",
    "    \n",
    "    files[\"train\"] = files['participant_id'].apply(train)\n",
    "    new_map = pd.DataFrame()\n",
    "    new_map[\"path\"] = files['path']\n",
    "    new_map[\"class\"] = files['class']\n",
    "    new_map[\"train\"] = files[\"train\"]\n",
    "    return new_map\n",
    "    \n",
    "new_map = label_test_train()\n",
    "new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over the map and split into x/y test/train\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m new_map\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     20\u001b[0m     path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m     class_item \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_map' is not defined"
     ]
    }
   ],
   "source": [
    "ROOT = 'data/staged/hand_data/'\n",
    "DROP_Z = True\n",
    "\n",
    "X_test = []\n",
    "X_train = []\n",
    "Y_test = []\n",
    "Y_train = []\n",
    "\n",
    "# Read in the parquet files as a Data Frame, Fill in NaN with Zero,\n",
    "# and drop Z axis from all landmarks \n",
    "def preprocess_data(full_path):\n",
    "    data = pd.read_parquet(full_path)\n",
    "    data.fillna(0.0, inplace=True)\n",
    "    if DROP_Z:\n",
    "        return data.loc[:, ~data.columns.str.startswith(\"z\")]\n",
    "    return data\n",
    "\n",
    "# Iterate over the map and split into x/y test/train\n",
    "for index, row in new_map.iterrows():\n",
    "    path = row[0]\n",
    "    class_item = row[1]\n",
    "    train = row[2]\n",
    "    \n",
    "    if train == 1:\n",
    "        full_path = ROOT + path\n",
    "        Y_train.append(class_item)\n",
    "        X_train.append(preprocess_data(full_path))\n",
    "    else: \n",
    "        full_path = ROOT + path\n",
    "        Y_test.append(class_item)\n",
    "        X_test.append(preprocess_data(full_path))\n",
    "\n",
    "# iterate over all the dataframes added to the X Train list and convert \n",
    "# them into lists of numpy arrays, then stack the list into an 3d numpy array\n",
    "X_train = [df.to_numpy() for df in X_train]\n",
    "X_train = np.stack(X_train, axis = 0)\n",
    "\n",
    "# iterate over all the dataframes added to the X Test list and convert \n",
    "# them into lists of numpy arrays, then stack the list into an 3d numpy array\n",
    "X_test = [df.to_numpy() for df in X_test]\n",
    "X_test = np.stack(X_test, axis = 0)\n",
    "\n",
    "# One Hot encode Y Train and Y test \n",
    "Y_train = to_categorical(Y_train, num_classes=NUM_CLASSES)\n",
    "Y_test = to_categorical(Y_test, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "# Reshape the 3d numpy array so we can normalize the data\n",
    "samples, frames, features = X_train.shape\n",
    "X_train_reshape = X_train.reshape((samples*frames, features))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_reshape)\n",
    "\n",
    "X_train = scaler.transform(X_train_reshape).reshape(samples, frames, features)\n",
    "\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0] * frames, features)\n",
    "X_test = scaler.transform(X_test_reshaped).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create1dCNN(X_train, NUM_CLASSES):\n",
    "\n",
    "    INPUT_SHAPE = X_train.shape[1:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(INPUT_SHAPE))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(), \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
